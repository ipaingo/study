import re
import random
import pymorphy2

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import matplotlib.lines as lines
import matplotlib.text as text
import matplotlib.cm as cm

#import seaborn as sns; sns.set()

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, StackingClassifier
from xgboost import XGBClassifier

from sklearn.model_selection import GridSearchCV
import itertools

import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Embedding, LSTM, GRU

# pd.options.display.width = 0
# pd.set_option('display.max_rows', 100)
# pd.set_option('display.max_columns', 11)

df = pd.read_csv("mails.csv", sep='\t')

cls_dic = {1:'–£—Å–ª–æ–≤–∏—è –ø–æ–¥–∞—á–∏',
           2:'–ü—Ä–æ—Ö–æ–¥–Ω–æ–π –∏ –¥–æ–ø—É—Å—Ç–∏–º—ã–π –±–∞–ª–ª',
           3:'–î–æ—Å—Ç–∏–∂–µ–Ω–∏—è',
           4:'–û–±—â–µ–∂–∏—Ç–∏—è',
           5:'–í—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å–ø—ã—Ç–∞–Ω–∏—è',
           6:'–ü–µ—Ä–µ–≤–æ–¥',
           7:'–ê—Å–ø–∏—Ä–∞–Ω—Ç—É—Ä–∞',
           8:'– –µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è',
           }
# hlt_dic = {1:'ONLINE –ø—Ä–∏—ë–º–Ω–∞—è',
#            2:'–û—á–Ω–∞—è –ø—Ä–∏–µ–º–Ω–∞—è',
#            3:'–ü—Ä–∏—ë–º–Ω–∞—è –∞—Å–ø–∏—Ä–∞–Ω—Ç—É—Ä—ã'
#            }

# df['cls_name'] = df['class'].map(lambda x: cls_dic[x])
# df['hlt_name'] = df.TYPE_HOTLINE.map(lambda x: hlt_dic[x])
# print(df)

# df["text"] = df["CONTENT"].map(lambda txt: txt.split())

def del_punct(word):
    return re.sub(r"\W", " ", word)

def filter_words(word):
    return word not in [
        "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ",
    ]

def morphan(word, morph):
    word = del_punct(word).strip()
    p = morph.parse(word)[0]

    word_new = word
    if (not 'Surn' in p.tag) and (not 'Name' in p.tag) and (not 'Patr' in p.tag) and ('NOUN' in p.tag):
        #—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –Ω–µ –§–ò–û
        word_new = p.normal_form
    elif 'Surn' in p.tag:
        word_new = '–§–ê–ú–ò–õ–ò–Ø'
    elif 'Name' in p.tag:
        word_new = '–ò–ú–Ø'
    elif 'Patr' in p.tag:
        word_new = '–û–¢–ß–ï–°–¢–í–û'


    elif ('INFN' in p.tag) or ('VERB' in p.tag): #–≥–ª–∞–≥–æ–ª
        word_new = p.normal_form

    elif ('ADJF' in p.tag) or ('ADJS' in p.tag) or ('COMP' in p.tag): #–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ
        word_new = p.normal_form


    elif ('PRTF' in p.tag) or ('PRTS' in p.tag) or ('GRND' in p.tag): #–ø—Ä–∏—á–∞—Å—Ç–∏–µ, –ø–æ—Ö–æ–∂–µ –Ω–∞ –≥–ª–∞–≥–æ–ª
        word_new = p.normal_form

    elif ('ADVB' in p.tag) or ('NPRO' in p.tag) or ('PRED' in p.tag) or ('PREP' in p.tag) or ('CONJ' in p.tag) or ('PRCL' in p.tag) or ('INTJ' in p.tag):
        word_new = ""

    elif ('NUMR' in p.tag) or ('NUMB' in p.tag) or ('intg' in p.tag): # —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ NUMB,intg
        word_new = ''

    else:
        word_new = word
    return word_new

def normtext(txt, morph):
    return str(' '.join(filter(filter_words, [morphan(x, morph) for x in txt.split()])))


def pymorphy2_311_hotfix():
    from inspect import getfullargspec
    from pymorphy2.units.base import BaseAnalyzerUnit

    def _get_param_names_311(klass):
        if klass.__init__ is object.__init__:
            return []
        args = getfullargspec(klass.__init__).args
        return sorted(args[1:])

    setattr(BaseAnalyzerUnit, '_get_param_names', _get_param_names_311)

pymorphy2_311_hotfix()
morph = pymorphy2.MorphAnalyzer()

df['text'] = df["CONTENT"].map(lambda x: normtext(x, morph))

print(df['text'])
with open("test.txt", "w", encoding="utf-8") as file:
    file.write(df["text"].to_csv())


def classifier(X_train, y_train, C=10.):
    '''
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
    '''

    tfv = TfidfVectorizer()
    X_train = tfv.fit_transform(X_train)

    clf = LogisticRegression(C=C)
    # clf = DecisionTreeClassifier()
    # clf = RandomForestClassifier()
    # clf = BaggingClassifier(DecisionTreeClassifier())
    # estimators = [  ('Forest', RandomForestClassifier()),
    #                 # ('XGBoost', XGBClassifier()),
    #                 ('Tree', DecisionTreeClassifier())]
    # estimators = [  ('Tree', DecisionTreeClassifier()),
    #                 ('Forest', RandomForestClassifier())]
    # clf = StackingClassifier(estimators=estimators)

    clf = clf.fit(X_train, y_train)

    return tfv, clf

def predictor(text, clf, tfv):
    '''
    text - –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–π —Ç–µ–∫—Å
    clf - –æ–±—É—á–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    tfv - –æ–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä

    '''
    X_test = tfv.transform([text])

    pred = clf.predict(X_test)

    return pred[0]

def try_to_determine(text):
    words = text.split()
    for word in words:
        if word in ["–∞—Å–ø–∏—Ä–∞–Ω—Ç—É—Ä–∞", "–∞—Å–ø–∏—Ä–∞–Ω—Ç", "–∫–∞–Ω–¥–∏–¥–∞—Ç"]:
            return 7
        if word in ["–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è"]:
            return 8
        # if word in ["–æ–±—â–µ–∂–∏—Ç–∏–µ"]: # –ü–æ—á–µ–º—É-—Ç–æ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –æ–±—â–µ–∂–∏—Ç–∏—é –∏–Ω–æ–≥–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —É—Å–ª–æ–≤–∏—è–º –ø–æ–¥–∞—á–∏. –ò –µ—â—ë 4 —Ä–∞–∑ –∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏.
        #     return 4              # –•–∞—Ö, –∑–∞–±–∞–≤–Ω–æ, —É–∫–∞–∑–∞–≤ —É—Å–ª–æ–≤–∏–µ –≤—ã—à–µ, –æ–±—â–µ–∂–∏—Ç–∏—è —Å–∫–∞–∫–Ω—É–ª–∏ —Å—Ä–∞–∑—É –¥–æ 1.00
    
    return 0

X_train, X_test, y_train, y_test = train_test_split(df.text, df['class'], random_state=42, test_size=0.3)
tfv, clf = classifier(X_train, y_train, C=10)
# tfv, clf = classifier(X_train, y_train, C=100)

class_save = []
pred = []
for nom, txt in enumerate(X_test.values):
    det_class = try_to_determine(txt)
    if det_class != 0:
        pred.append(det_class)
    else:
        pred.append(predictor(txt, clf, tfv))

y_test_list = y_test.tolist()
pred_list = pred

mtrs = classification_report([cls_dic[x] for x in y_test_list], [cls_dic[x] for x in pred_list])
print(mtrs)





# X_train_1, X_vt_1, y_train_1, y_vt_1 = train_test_split(df.text, df['class'], random_state=42, test_size=0.2)
# X_valid_1, X_test_1, y_valid_1, y_test_1 = train_test_split(X_vt_1, y_vt_1, test_size=0.5, random_state=42)

# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(X_train_1)

# textSequences = tokenizer.texts_to_sequences(X_train_1)

# max_words = 0
# for desc in df.text.tolist():
#     words = len(desc.split())
#     if words > max_words:
#         max_words = words
# print('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Å–∞–º–æ–º –¥–ª–∏–Ω–Ω–æ–º –ø–∏—Å—å–º–µ: {} —Å–ª–æ–≤'.format(max_words))

# total_unique_words = len(tokenizer.word_counts)
# print('–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {}'.format(total_unique_words))

# maxSequenceLength = max_words

# vocab_size = 10000
# tokenizer = Tokenizer(num_words=vocab_size)
# tokenizer.fit_on_texts(df.text)

# X_train = tokenizer.texts_to_sequences(X_train_1)
# X_valid = tokenizer.texts_to_sequences(X_valid_1)
# X_test = tokenizer.texts_to_sequences(X_test_1)

# X_train = sequence.pad_sequences(X_train, maxlen=maxSequenceLength)
# X_valid = sequence.pad_sequences(X_valid, maxlen=maxSequenceLength)
# X_test = sequence.pad_sequences(X_test, maxlen=maxSequenceLength)

# print('– –∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X_train:', X_train.shape)
# print('– –∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X_valid:', X_valid.shape)
# print('– –∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X_test:', X_test.shape)

# # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –º–∞—Ç—Ä–∏—Ü—É –¥–≤–æ–∏—á–Ω—ã—Ö —á–∏—Å–µ–ª (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è categorical_crossentropy)

# num_classes = df['class'].unique().shape[0]+1

# y_train = keras.utils.to_categorical(y_train_1, num_classes)
# y_valid = keras.utils.to_categorical(y_valid_1, num_classes)
# y_test = keras.utils.to_categorical(y_test_1, num_classes)
# print('y_train shape:', y_train.shape)
# print('y_valid shape:', y_valid.shape)
# print('y_test shape:', y_test.shape)

# print(u'–°–æ–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å...')
# model = Sequential()
# # model.add(Embedding(10000, maxSequenceLength))
# model.add(Embedding(1500, maxSequenceLength))
# # model.add(Embedding(100000, maxSequenceLength))

# # model.add(LSTM(32, dropout=0.3, recurrent_dropout=0.3))
# # model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1))
# # model.add(LSTM(32, dropout=0.0, recurrent_dropout=0.0))
# model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))
# # model.add(GRU(32, dropout=0.2, recurrent_dropout=0.2))

# # model.add(Dense(num_classes, activation='sigmoid'))
# # model.add(Dense(num_classes, activation='silu'))
# # model.add(Dense(num_classes, activation='exponential'))
# model.add(Dense(num_classes, activation='softmax'))

# model.compile(loss='categorical_crossentropy',
# # model.compile(loss='poisson',
# # model.compile(loss='categorical_focal_crossentropy',
#               optimizer='adam',
#             #   optimizer='adamw',
#             #   optimizer='nadam',
#               metrics=['accuracy'])

# print(model.summary())

# batch_size = 32
# epochs = 25

# print(u'–¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...')
# history = model.fit(X_train, y_train,
#           batch_size=batch_size,
#           epochs=epochs,
#           validation_data=(X_valid, y_valid))

# predictions = model.predict(X_test).argmax(axis=1)
# y2 = np.array(y_test_1.to_list())
# pred2 = np.array(predictions)

# print(classification_report([cls_dic[x] for x in y2], [cls_dic[x] for x in pred2]))